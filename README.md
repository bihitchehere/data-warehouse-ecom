# ğŸ§± E-commerce Data Warehouse â€” End-to-End ELT Pipeline  
**PostgreSQL â†’ DuckDB | Python | Pandas | Dimensional Modeling**

---

## ğŸ“– Project Overview

This repository documents and implements a **complete, step-by-step build of an analytical data warehouse** for an e-commerce platform.

The project simulates a **real-world data engineering workflow**:
- Operational data is generated and stored in **PostgreSQL (OLTP)**
- Data is extracted and transformed using **Python & Pandas**
- Cleaned, analytics-ready data is loaded into **DuckDB (OLAP)**
- A **Kimball-style Star Schema** enables fast SQL-based business reporting

The goal is to demonstrate **core data engineering fundamentals**:  
data modeling, ELT design, SQL analytics, and warehouse performance â€” without unnecessary cloud complexity.

---

## ğŸ—ï¸ Architecture Overview

![Architecture Diagram](assets/architecture.png)

**Pipeline flow (left â†’ right):**

1. **Data Sources**
   - PostgreSQL transactional database
   - CSV batch files (simulated external inputs)

2. **Python ELT Layer**
   - Pandas-based extraction
   - Data cleaning and normalization
   - Revenue calculations and date handling

3. **OLAP Warehouse (DuckDB)**
   - File-based analytical database
   - Star Schema (Fact + Dimensions)
   - Optimized for analytical SQL queries

4. **Business Intelligence**
   - SQL aggregations
   - CSV outputs ready for BI tools (Tableau / Power BI)

---

## ğŸ§  Business Domain & Metrics

**Domain:** E-commerce platform  
**Fact grain:** One row per **order line item**

### Key Business Questions Answered
- How much revenue do we generate per day?
- How many orders are placed daily?
- Which products generate the most revenue?
- How many active users do we have per day?

### Core Metrics
- Daily revenue
- Daily order count
- Top products by revenue
- Daily Active Users (DAU)

All metrics are computed directly from the **fact table** using SQL.

---

## â­ Data Modeling Approach

The warehouse follows a **Kimball-style Star Schema**.

### Dimension Tables
- `dim_users` â€” customer attributes
- `dim_products` â€” product catalog
- `dim_date` â€” calendar dimension

### Fact Table
- `fact_orders`
  - order_id
  - user_id
  - product_id
  - order_date
  - quantity
  - revenue

This structure enables:
- Simple joins
- Fast aggregations
- Clear analytical semantics

---

## ğŸš€ Technical Stack

| Layer | Technology |
|-----|-----------|
| Language | Python 3.10+ |
| OLTP Database | PostgreSQL |
| OLAP Warehouse | DuckDB |
| Data Processing | Pandas |
| Data Modeling | Star Schema (Kimball) |
| Orchestration | Python scripts |
| Analytics | SQL |
| Reporting | CSV exports (BI-ready) |

---

## ğŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ data/                  # Raw CSV inputs
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ metrics.md         # Business metrics & definitions
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ oltp_schema.sql    # PostgreSQL schema (OLTP)
â”‚   â””â”€â”€ olap_schema.sql    # DuckDB star schema (OLAP)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py   # Synthetic data generation (Faker)
â”‚   â”œâ”€â”€ etl.py             # ELT pipeline (Postgres â†’ DuckDB)
â”‚   â””â”€â”€ aggregations.py   # Business metrics & reports
â”œâ”€â”€ reports/               # Final analytical CSV outputs
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ architecture.png  # System architecture diagram
â”œâ”€â”€ olap.duckdb            # Analytical warehouse (gitignored)
â””â”€â”€ README.md
ğŸ”„ End-to-End Pipeline Flow
1ï¸âƒ£ Data Generation
Synthetic users, products, orders, and order items

Generated using Faker

Loaded into PostgreSQL

2ï¸âƒ£ Extract
Tables extracted from PostgreSQL using Pandas

Data loaded into memory for processing

3ï¸âƒ£ Transform
Orders joined with order items

Revenue calculated (quantity Ã— price)

Dates normalized and typed correctly

4ï¸âƒ£ Load
Dimension tables loaded into DuckDB

Fact table populated with analytics-ready data

Star Schema created inside DuckDB

ğŸ“ˆ Analytics & Outputs
The aggregations.py script produces:

ğŸ“Š Daily revenue trends

ğŸ§¾ Daily order counts

ğŸ›’ Top 10 products by revenue

ğŸ‘¥ Daily Active Users (DAU)

All results are exported as CSV files to /reports
and can be directly connected to BI tools like Tableau.

ğŸ› ï¸ Setup & Usage
1ï¸âƒ£ Clone Repository
bash
Copy code
git clone https://github.com/yourusername/ecommerce-data-warehouse.git
cd ecommerce-data-warehouse
2ï¸âƒ£ Create & Activate Virtual Environment
bash
Copy code
python -m venv venv ( or if you have anaconda  ) 
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Generate Source Data
bash
Copy code
python scripts/generate_data.py
5ï¸âƒ£ Run ELT Pipeline
bash
Copy code
python scripts/etl.py
python scripts/aggregations.py
ğŸ¯ Why This Project Matters
This project demonstrates:

âœ… Realistic ELT patterns (not toy examples)

âœ… Proper analytical data modeling

âœ… SQL-first analytics

âœ… Clear separation of OLTP vs OLAP

âœ… Strong foundation for cloud warehouses (Snowflake / Databricks / Azure)

It mirrors how modern data teams actually build analytical systems.

ğŸ”® Possible Extensions
Incremental loads

dbt transformations

Cloud storage (Azure Data Lake / S3)

Orchestration (Airflow / Databricks Jobs)

Data quality tests

ğŸ‘¤ Author
Brahim askiou 
Data Engineer / Analytics Engineer
